---
title: "Homework 4"
author: "Denis Ostroushko"
format: 
  pdf:
    toc: false
    number-sections: false
    colorlinks: true
editor: source
execute: 
  eval: true
  echo: false
  message: false
  warning: false
---

```{r}
library(tidyverse)
library(kableExtra)
library(gridExtra)
```

# Problem 1



```{R}

fpath <- "https://jaredhuling.org/data/pubh7406/thall_and_vail_1990.dat"

seiz <- read.table(fpath, header=TRUE)

## sum up seizures across visits
seiz_total <- 
  seiz %>%
    group_by(id) %>%
    dplyr::summarize(
      seiz = sum(seiz),
      age = age[1],
      base = base[1],
      log_base = log(base/4),
      treat = treat[1]
      )

```

### (1)

First, we will fit the two model and display the code so that you may follow along: 

```{r}
#| echo: true
f1 <- glm(seiz ~ log(age)+log_base+treat, data = seiz_total, family = poisson())
fq1 <- glm(seiz ~ log(age)+log_base+treat, data = seiz_total, family = quasipoisson())
```

Using the diagonal of a variance-covariance matrix we can obtain variances of each model coefficient. 

Variances of fitted coefficients from the Poisson regression model are given below: 

```{r}
#| echo: true
diag(vcov(f1))
```

Variances of fitted coefficients from the Quasipoisson regression model are given below: 

```{r}
#| echo: true
diag(vcov(fq1))
```

Taking the ratio of the two coefficient vectors we obtain a constant value of the over-dispersion parameter

```{r}
#| echo: true
diag(vcov(fq1)) / diag(vcov(f1))
```

Note that the summary output of the Quasipoisson regression model provides the same value: 

```{r}
summary(fq1)
```




### (2)

```{r}
N_iter <- 5000
```

Using code below with `N_iter` iterations we will obtain bootstrapp sampling distributions of Poisson regression 
model coefficients. 

```{r}
#| echo: true

results <- 
  data.frame(
    i = seq(1,N_iter, by = 1), 
    intercept_b = rep(NA, N_iter),
    log_age_b = rep(NA, N_iter), 
    log_base_b = rep(NA, N_iter), 
    treat_b = rep(NA, N_iter)
  )

set.seed(1)

for(i in 1:N_iter){
  
  temp_m <- glm(seiz ~ log(age)+log_base+treat, 
                data = seiz_total[sample(rownames(seiz_total), replace= T), ], 
                family = poisson())
  
  results$intercept_b[i] = coef(temp_m)[1]
  results$log_age_b[i] = coef(temp_m)[2]
  results$log_base_b[i] = coef(temp_m)[3]
  results$treat_b[i] = coef(temp_m)[4]

}

```

@tbl-coef-comp-1 shows comparison of standard errors for model term coefficients obtained using three 
different ways. 

```{r}
#| label: tbl-coef-comp-1
#| tbl-cap: "Comparion of fitted and boostrapped model parameters"
#| 

add_model_coef <- 
  
  results %>% 
    summarize(
      across(
        .cols = c('log_age_b', 'log_base_b', 'treat_b'), 
        .fns = list(mean, sd)
    )) 

mean_add <- add_model_coef[c(1,3,5)]
var_names <- names(mean_add)
names(mean_add) <- NULL

sd_add <- add_model_coef[c(2,4,6)]
names(sd_add) <- NULL

add_model_coef_f <- 
  data.frame(
    names = var_names, 
    add_mean = t(mean_add) , 
    add_sd = t(sd_add)
  )

add_model_coef_f <- 
  add_model_coef_f %>% 
  
  mutate(
    pretty_names = c("Log(Age)", "Log(Baseline Seizures)", "Treatment")
  ) %>% 
  
  select(-names) 

add_model_coef_f$fitted_vals <- coef(f1)[-1]
add_model_coef_f$fitted_se <- coefficients(summary(f1))[-1,2]


add_model_coef_f$fitted_vals_q <- coef(fq1)[-1]
add_model_coef_f$fitted_se_q <- coefficients(summary(fq1))[-1,2]

add_model_coef_f <- 
  add_model_coef_f %>% 
  select(
    pretty_names, 
    
    fitted_vals, fitted_se,
    
    fitted_vals_q, fitted_se_q, 
    
    add_mean, add_sd
    
  )

add_model_coef_f %>% 
  
  kable(booktabs = T, 
    #    caption = "Comparion of fitted and boostrapped model parameters", 
        linesep = "\\addlinespace", 
        align = c('l', rep('c', (length(add_model_coef_f)-1))), 
        digits = 2, 
        col.names = c("Model Terms",  
                      "Beta", "SE", 
                      "Beta", "SE", 
                      "Avg. Beta", "SE"
                      )) %>% 
  kable_styling(latex_options = c("condenced", "striped", "hold_position"), 
                full_width = F) %>% 
  column_spec(2:length(add_model_coef_f), width = "1cm") %>%
  column_spec(1, width = "4cm") %>% 
  add_header_above(c(" " = 1,
                     "Poisson \nFitted Values" = 2, 
                     "Quasipoisson \nFitted Values" = 2, 
                     "Boostrapp \nSimple Model" = 2
                     ))

```

Comments: 

1. We can see full effect of Poisson regression deficiency, standard errors for beta's are much much smaller 
for Possion beta's when compared to Quasipoisson. 

2. Quasipoisson and Bootstrap standard errors are *approximately* equal. For some variables bootstrap standard 
error is bigger, for some quasipoisson standard errors are higher. 

3. Overall, Quasipoisson and Boostrap methods produce similar standard errors, so either method should be 
appropriate for use in practice. 



### (3)

Bias = average Beta's from `r `N_iter` bootstrap iterations - fitted coefficients from regression model. 

Summarize bootstrap iterations data and present results: 

```{r}
#| echo: true
boot_beta <- 
  results %>% 
    summarise(across(.cols = c("intercept_b", "log_age_b", "log_base_b", "treat_b"), .fns = mean))

boot_beta
```

True coefficients from the regression model: 

```{r}
coef(f1)
```

Taking the difference produces bias for each model coefficient: 
```{R}
#| echo: true
boot_beta - coef(f1)

```
Numerically, bias estimates are very close to 0 for each model term, so we have no reason to believe that the 
estimates are biased. 

### (4)

```{R}

sampling_dist_plot <- 
  function(res_data, variable){
         
      pretty_name <- 
        case_when(
          variable == 'log_age_b' ~ "Log( Age )", 
          variable == 'log_base_b' ~ "Log( Baseline Seizures )", 
          variable == 'treat_b' ~ "Treatment", 
          variable == 'log_age_log_base_int_b' ~ "Log(Age):Log(Base. Seizures) Interaction",
          variable == 'log_age_treat_int_b' ~ "Log(Age):Treatment Interaction",
          variable == 'log_base_treat_b' ~ "Log(Base. Seizures):Treatment Interaction",
          variable == 'log_age_log_base_treat_int_b' ~ "Three-way Interaction"
        )
    
      p1 <- ggplot(data = res_data, 
             aes_string(x = variable)) + 
        geom_histogram(
          aes(y =..density..),
          bins = 20 , 
          fill = "grey", 
          color = "black"
        ) + 
      
      stat_function(fun = dnorm, args = list(mean = mean(res_data[,variable]), 
                                             sd = sd(res_data[,variable])), 
                    aes(color = "Normal Distribution \nwith Mean and Variance \nfrom Observed Data"), 
                    size = 1
                    ) + 
      theme_minimal() + 
      
      scale_color_discrete(name = "") + 
      
      ylab("") + 
      xlab(
        paste0("Sampling Distribtuion of\n ", pretty_name, " coefficient")
        ) + 
      theme(legend.position="bottom")
      
      p2 <-  ggplot(res_data, aes_string(sample = variable)) + stat_qq() + stat_qq_line() + theme_minimal() +
        theme(legend.position = "bottom")
      
      grid.arrange(p1, p2, nrow = 1) 
       
  }

```

We will use function displayed below to obtain normal approximation confidence intervals: 

```{r}
#| echo: true
normal_approx <- 
  function(coef, boot_se, rounding = 5){
    
    vec = c(
      round(coef - qt(p = 0.975, df = nrow(seiz_total) - 1) * boot_se , rounding),
      round(coef + qt(p = 0.975, df = nrow(seiz_total) - 1) * boot_se , rounding)
    )
    
   names(vec) <- c("2.5%", "97.5%")
    
   return(vec)
  }

```

I will use one page per variable in this and future sections for organization purposes. 

\newpage

**Log (Age)**

@fig-log-age shows bootstrap sampling distribution for log(age) predictor from the simple additive model. 
We use mean and variance of bootstrap sampling distribution to fit and display a normal curve. 

Visually, log(age) distribution looks approximately normal. 

```{r}
#| fig-cap: "Sampling Distribution of Log(Age) regression coefficient" 
#| label: fig-log-age
#| fig-width: 8

sampling_dist_plot(res_data = results, variable = 'log_age_b')
```

1. Normal Approximation Method

```{r}
normal_approx(coef = coef(f1)[2], boot_se = sd(results$log_age_b))
```

2. Percentile Method

```{r}
quantile(results$log_age_b, p = c(0.025, 0.975))
```

3. Comparison with Quasipoisson

```{r}
confint(fq1)[2,]
```

4. Comments: 

* We can see that Quasipoisson produces the widest itnerval in the set of three options. However, 
it is well within a range that we might want to consider as noise, without conducting more tests to verify 
the difference. 

* Normal approximation is slightly wider than the quantile interval method. We can see that the normal curve and 
historgram both show slight evidence of heavy tails or values that are starting to be a little extreme. 
Greater width of distribution increases variance, and therefore we have a higher standard error for the 
normal approximation confidence interval. 

\newpage

**Log (Baseline Seizures)**


@fig-log-seiz shows bootstrap sampling distribution for log(age) predictor from the simple additive model. 

Visually, log(age) distribution does not look approximately normal. 
In fact, it looks more bimodal. QQplot also shows evidence of deviation from normality for the sampling
distribution. 

```{r}
#| fig-cap: "Sampling Distribution of Log(Baseline Seizures) regression coefficient" 
#| label: fig-log-seiz
#| fig-width: 8
sampling_dist_plot(res_data = results, variable = 'log_base_b')
```

1. Normal Approximation Method

```{r}
normal_approx(coef = coef(f1)[3], boot_se = sd(results$log_base_b))
```

2. Percentile Method

```{r}
quantile(results$log_base_b, p = c(0.025, 0.975))
```

3. Comparison with Quasipoisson

```{r}
confint(fq1)[3,]
```

4. Comments: 

* All three methods produce approximately similar confidence intervals. However, due to deviation from normality,
we can no longer interpret these confidence intervals in a regular way. 

\newpage 

**Treatment**

```{r}
#| fig-cap: "Sampling Distribution of Treatment regression coefficient" 
#| label: fig-treat
#| fig-width: 8
sampling_dist_plot(res_data = results, variable = 'treat_b')
```

1. Normal Approximation Method

```{r}
normal_approx(coef = coef(f1)[4], boot_se = sd(results$treat_b))
```

2. Percentile Method

```{r}
quantile(results$treat_b, p = c(0.025, 0.975))
```

3. Comparison with Quasipoisson

```{r}
confint(fq1)[4,]
```

4. Comments: 

* Normal approximation confidence interval is quite wider than the other two methods. We can see that both tails 
are quite heavier than expected under the normal distribution. Therefore, variance of the estimate and standard 
error appear more inflated. Quantiles are less sensitive to more extreme values, therefore, quantile 
method aligns with the model estiamted confidence interval better.

\newpage

### (5)

```{r}

int_model <- glm(seiz ~ log(age)*log_base*treat, data = seiz_total, family = poisson())
quasi_int_model <- glm(seiz ~ log(age)*log_base*treat, data = seiz_total, family = quasipoisson())

```

Repeat a bootstrap procedure with a model containing a total of eight predictors now. 

```{r}
#| echo: true
#| 
results_int <- 
  data.frame(
    i = seq(1,N_iter, by = 1), 
    intercept_b = rep(NA, N_iter),
    log_age_b = rep(NA, N_iter), 
    log_base_b = rep(NA, N_iter), 
    treat_b = rep(NA, N_iter),
    log_age_log_base_int_b = rep(NA, N_iter),
    log_age_treat_int_b = rep(NA, N_iter), 
    log_base_treat_b = rep(NA, N_iter), 
    log_age_log_base_treat_int_b = rep(NA, N_iter)
    
  )

set.seed(1)

for(i in 1:N_iter){
  
  temp_m <- glm(seiz ~log(age)*log_base*treat, 
                data = seiz_total[sample(rownames(seiz_total), replace= T), ], 
                family = poisson())
  
  results_int$intercept_b[i] = coef(temp_m)[1]
  results_int$log_age_b[i] = coef(temp_m)[2]
  results_int$log_base_b[i] = coef(temp_m)[3]
  results_int$treat_b[i] = coef(temp_m)[4]
  results_int$log_age_log_base_int_b[i] = coef(temp_m)[5]
  results_int$log_age_treat_int_b[i] = coef(temp_m)[6]
  results_int$log_base_treat_b[i] = coef(temp_m)[7]
  results_int$log_age_log_base_treat_int_b[i] = coef(temp_m)[8]

}


```

**Comparison with simpler additive model** 

@tbl-coef-comp displays fitted coefficients and standard errors for a more complicated model, as well as a simpler poisson 
regression. 

```{R}
#| label: tbl-coef-comp
#| tbl-cap: "Comparion of fitted and boostrapped model parameters"
#| 
###################
# coefficients and standard errors from bootsrap from the interative model 

int_model_coef <- 
  
  results_int %>% 
    summarize(
      across(
        .cols = c('log_age_b', 'log_base_b', 'treat_b', 
                  'log_age_log_base_int_b', 'log_age_treat_int_b', 
                  "log_base_treat_b",'log_age_log_base_treat_int_b'), 
        .fns = list(mean, sd)
    )) 

mean_int <- int_model_coef[seq(from = 1, to = (length(int_model_coef)-1), by = 2)]
var_names <- names(mean_int)
names(mean_int) <- NULL

sd_int <- int_model_coef[seq(from = 2, to = (length(int_model_coef)), by = 2)]
names(sd_int) <- NULL

int_model_coef_f <- 
  data.frame(
    names = var_names, 
    int_mean = t(mean_int) , 
    int_sd = t(sd_int)
  )


####################
# coefficients and standard errors from boostrapp from the additive model 


add_model_coef <- 
  
  results %>% 
    summarize(
      across(
        .cols = c('log_age_b', 'log_base_b', 'treat_b'), 
        .fns = list(mean, sd)
    )) 

mean_add <- add_model_coef[c(1,3,5)]
var_names <- names(mean_add)
names(mean_add) <- NULL

sd_add <- add_model_coef[c(2,4,6)]
names(sd_add) <- NULL

add_model_coef_f <- 
  data.frame(
    names = var_names, 
    add_mean = t(mean_add) , 
    add_sd = t(sd_add)
  )

coef_from_two_model <- 
  int_model_coef_f %>% 
  left_join(
    add_model_coef_f, 
    by = "names", 
    all = T
  ) %>% 
  
  mutate(
    pretty_names = c("Log(Age)", "Log(Baseline Seizures)", "Treatment", "Log(Age):Log(Baseline Seizures)", 
                     "Log(Age):Treatment", "Log(Baseline Seizures_:Treatment",
                     "Log(Age):Log(Baseline Seizures):Treatment")
  ) %>% 
  
  select(-names) 

coef_from_two_model$fitted_vals <- coef(int_model)[-1]
coef_from_two_model$fitted_se <- coefficients(summary(int_model))[-1,2]


coef_from_two_model$fitted_vals_q <- coef(quasi_int_model)[-1]
coef_from_two_model$fitted_se_q <- coefficients(summary(quasi_int_model))[-1,2]

coef_from_two_model <- 
  coef_from_two_model %>% 
  select(
    pretty_names, 
    
    fitted_vals, fitted_se,
    
    fitted_vals_q, fitted_se_q, 
    
    int_mean, int_sd, 
    
    add_mean, add_sd
    
  )

coef_from_two_model %>% 
  
  kable(booktabs = T, 
    #    caption = "Comparion of fitted and boostrapped model parameters", 
        linesep = "\\addlinespace", 
        align = c('l', rep('c', (length(coef_from_two_model)-1))), 
        digits = 2, 
        col.names = c("Model Terms",  
                      "Beta", "SE", 
                      "Beta", "SE", 
                      "Avg. Beta", "SE",
                      "Avg. Beta", "SE"
                      )) %>% 
  kable_styling(latex_options = c("condenced", "striped", "hold_position"), 
                full_width = F) %>% 
  column_spec(2:length(coef_from_two_model), width = "1cm") %>%
  column_spec(1, width = "4cm") %>% 
  add_header_above(c(" " = 1,
                     "Poisson \nFitted Values" = 2, 
                     "Quasipoisson \nFitted Values" = 2, 
                     "Bootsrapp Model \nwith Interaction" = 2, 
                     "Boostrapp \nSimple Model" = 2
                     ))


```

Comments: 

* We obviously know that a Poisson regression model will severely underestimate variance. For some coefficients the difference 
is absolutely enormous. 

* This example showcases how bootstrap standard error estimates are consistently lower than the quasipoisson estimates. 
Jared pointed out that underestimation of standard errors is a known flaw of bootstrap methods. 

* It was surprising for me to see that even after `r N_iter` replications, we get a notable discrepancy between quasipoisson 
regression coefficients and average Beta's from the replications. I am not sure if this suggests that the model estimates are 
biased. Mathematically, we surely will see a notable amount of bias. 

\newpage

**Log (Age)**

```{r}
#| fig-cap: "Sampling Distribution of Log(Age) regression coefficient from a model with interaction terms" 
#| label: fig-log-age-int
#| fig-width: 8
sampling_dist_plot(res_data = results_int, variable = 'log_age_b')
```

1. Normal Approximation Method

```{r}
normal_approx(coef = coef(int_model)[2], boot_se = sd(results_int$log_age_b))
```

2. Percentile Method

```{r}
quantile(results_int$log_age_b, p = c(0.025, 0.975))
```

3. Comparison with Quasipoisson

```{r}
confint(quasi_int_model)[2,]
```

4. Comments: 

* I wouldn't particularly call this sampling distribution approximately normal. Lower tail is quite heavy, 
so normal and quantile approximations of the confidence interval might not make sense anymore. Complicated models commonly 
have such issues and deviations from assumptions. 

* Normal approximation method is heavily affected by the variance inflation from lower tail outliers. 

* Quantile method is affected by deviation from normality: observed quantiles of the data do not align with theoretical quantiles of a normal distribution

5. Differences: 

* The distribution has a heavy hail now, which we did not see on @fig-log-age. 

* Confidence interval endpoints are now quite different as well. Skewness of the distribution affects estiamtes. 

\newpage

**Log (Baseline Seizures)**

```{r}
#| fig-cap: "Sampling Distribution of Log(Base) regression coefficient from a model with interaction terms" 
#| label: fig-log-base-int
#| fig-width: 8
sampling_dist_plot(res_data = results_int, variable = 'log_base_b')
```

1. Normal Approximation Method

```{r}
normal_approx(coef = coef(int_model)[3], boot_se = sd(results_int$log_base_b))
```

2. Percentile Method C.I. 

```{r}
quantile(results_int$log_base_b, p = c(0.025, 0.975))
```

3. Comparison with Quasipoisson

```{r}
confint(quasi_int_model)[3,]
```

4. Comments and Comparisons: 

* More complicated model and interaction of factors changed the distribution of log-baseline seizures. The distribution is
no longer bimodal. 

* However, I would say there are still issues with the sampling distribution. Heavy tails cause the discrepancy between the 
three methods we use to obtain confidence intervals. 


\newpage 

**Treatment**

```{r}
#| fig-cap: "Sampling Distribution of Treatment regression coefficient from a model with interaction terms" 
#| label: fig-treat-int
#| fig-width: 8
sampling_dist_plot(res_data = results_int, variable = 'treat_b')
```

1. Normal Approximation Method

```{r}
normal_approx(coef = coef(int_model)[4], boot_se = sd(results_int$treat_b))
```

2. Percentile Method C.I. 

```{r}
quantile(results_int$treat_b, p = c(0.025, 0.975))
```

3. Comparison with Quasipoisson

```{r}
confint(quasi_int_model)[4,]
```

4. Comments: 

* Treatment flag sampling distribution looks approximately normal, with a minor issue at the lower tail 

* It seems to me that the range of normal approximation and percentile methods are very similar, however, the 
bounds are quite different. I suspect that this goes back to the bias comment I made easrlier. Normal approximation uses 
a true fitted value of a coefficient and bootstrap standard error to make an interval. Quantile method relies purely on the 
estimated sampling distribution, which has a different average - center - which is a the average Beta from all replications. 
The difference in what we consider the center of the interval causes differing bounds between the two methods 

* Quasipoisson method produces a wider confidence interval, and it is wider equally in each direction. 

5. Comparisons: 

* Everything is different for treatment now. Fitted coefficient is quite large and far from zero

* Confidence interval is also extremely wide when compared with he previous results from a simpler model 

* It is hard to say what exactly is the cause of such a large discrepancy without examining how the model behavior 
changes with addition of each new variable and interaction. 

\newpage 

**Log(Age) - Log(Baseline Seizures) Interaction Term**

```{r}
#| fig-cap: "Sampling Distribution of respective interactive term regression coefficient" 
#| label: fig-age-base-int
#| fig-width: 8
sampling_dist_plot(res_data = results_int, variable = 'log_age_log_base_int_b')
```

1. Normal Approximation Method

```{r}
normal_approx(coef = coef(int_model)[5], boot_se = sd(results_int$log_age_log_base_int_b))
```

2. Percentile Method C.I. 

```{r}
quantile(results_int$log_age_log_base_int_b, p = c(0.025, 0.975))
```

3. Comparison with Quasipoisson

```{r}
confint(quasi_int_model)[5,]
```

4. Comments: 

* Similar issue - heavy tails of sampling distribution. 

* Despite the issue we have confidence interval endpoints that are quite similar. We saw that this is not always the case 
form previous examinations, so we will treat this occurrence as a coincidence. 

\newpage 

**Treatment - Log(Age) Interaction Term**

```{r}
#| fig-cap: "Sampling Distribution of respective interactive term regression coefficient" 
#| label: fig-treat-age-int
#| fig-width: 8
sampling_dist_plot(res_data = results_int, variable = 'log_age_treat_int_b')
```

1. Normal Approximation Method

```{r}
normal_approx(coef = coef(int_model)[6], boot_se = sd(results_int$log_age_treat_int_b))
```

2. Percentile Method C.I. 

```{r}
quantile(results_int$log_age_treat_int_b, p = c(0.025, 0.975))
```

3. Comparison with Quasipoisson

```{r}
confint(quasi_int_model)[6,]
```

4. Comments: 

* We can see that there is an issue at the upper tail, the tail is quite heavier than expected. 

* This issue causes the discrepancy between quantile and normal approximation methods. 

* Quasipoisson confidence interval is wider in each direction again. 

\newpage 

**Treatment - Log(Baseline Seizures) Interaction Term**

```{r}
#| fig-cap: "Sampling Distribution of respective interactive term regression coefficient" 
#| label: fig-treat-base-int
#| fig-width: 8
sampling_dist_plot(res_data = results_int, variable = 'log_base_treat_b')
```

1. Normal Approximation Method

```{r}
normal_approx(coef = coef(int_model)[7], boot_se = sd(results_int$log_base_treat_b))
```

2. Percentile Method C.I. 

```{r}
quantile(results_int$log_base_treat_b, p = c(0.025, 0.975))
```

3. Comparison with Quasipoisson

```{r}
confint(quasi_int_model)[7,]
```

4. Comments: 

* We can see that there is an issue at the upper tail, the tail is heavier than expected. 

* This issue causes the discrepancy between quantile and normal approximation methods. 

* Quasipoisson confidence interval is wider in each direction again. 

\newpage 

**Three-variable Interaction Term**

```{r}
#| fig-cap: "Sampling Distribution of a three-variable interaction term regression coefficient" 
#| label: fig-threesome-int
#| fig-width: 8
sampling_dist_plot(res_data = results_int, variable = 'log_age_log_base_treat_int_b')
```

1. Normal Approximation Method

```{r}
normal_approx(coef = coef(int_model)[8], boot_se = sd(results_int$log_age_log_base_treat_int_b))
```

2. Percentile Method C.I. 

```{r}
quantile(results_int$log_age_log_base_treat_int_b, p = c(0.025, 0.975))
```

3. Comparison with Quasipoisson

```{r}
confint(quasi_int_model)[8,]
```


4. Comments: 

* We can see that there is an issue at the lower tail, the tail is heavier than expected. 

* Despite the issue we have confidence interval endpoints that are quite similar. We saw that this is not always the case 
form previous examinations, so we will treat this occurrence as a coincidence. 

* Quasipoisson confidence interval is wider in each direction again. 

\newpage 

# Problem 2

```{r}
fish <- read.table('http://jaredhuling.github.io/data/fish.txt', header=TRUE)
```

### (i)

I am using code below to obtain difference curves for various values of $\beta_1$ ad $\beta_2$

```{r}
#| echo: true
bh <- function(S, beta1, beta2){
  
  1/(beta1 + beta2/S)
  
}
```

@fig-bh shows combinations of different parameter values and the curves that they produce. On the original scale of $R$ and 
$S$, none of these curves seem like a good fit to the data. 

```{R}
#| label: fig-bh
#| fig-cap: "Various B-H curves"

bh_data <- 
  fish %>% 
  
  mutate(curve1 = bh(S = S, beta1 = .002, beta2 = .7),
         curve2 = bh(S = S, beta1 = .002, beta2 = 1),
         curve3 = bh(S = S, beta1 = .005, beta2 = 1))

ggplot(data = fish, 
       aes(x = R, y = S ) )+ 
  geom_point() + 
  geom_smooth(data = bh_data, aes(x = S, y = curve1, color = "B1 = 0.002, B2 = 0.7")) +
  geom_smooth(data = bh_data, aes(x = S, y = curve2, color = "B1 = 0.002, B2 = 1.0")) + 
  geom_smooth(data = bh_data, aes(x = S, y = curve3, color = "B1 = 0.005, B2 = 1.0")) + 
  
  theme_minimal() 

```

### (ii)

Using code below we fit the mode. Summary is also provided. We will use `coef()` function later ro complete future tasks.

```{r}
#| echo: true
bh_lm <- lm(I(1/R) ~ I(1/S), data = fish)

summary(bh_lm)

```

### (iii)

Using a formula for the stable population that Jared provided we can write `R` code to compute this value

```{r}
#| echo: true
N = (1 - coef(bh_lm)[2])/coef(bh_lm)[1]

N

```

It appears that `r floor(N)` or `r ceiling(N)` is the number of fish in each class required to maintain a stable population. 

### (iv)

```{r}
set.seed(1)
```

All bootstrap work will be done with 1000 replications in the following sections. 

Bootstrapped stable population estimate is provided using this function below: 

```{r}
#| echo: true
N_hat <- function(df, est_iter) {

  res <- 
    data.frame(
      i = seq(1,est_iter, 1), 
      N_Stable = rep(NA, est_iter)
    )
  
  for(i in 1:est_iter){
    iter_bh_lm <- lm(I(1/R) ~ I(1/S), data = df[sample(rownames(df), replace = T), ])
    
    res$N_Stable[i] = (1 - coef(iter_bh_lm)[2])/coef(iter_bh_lm)[1]
  }
  
  estimate = mean(res$N_Stable)
  
  return(estimate)
}

N_hat(df = fish, est_iter = 1000)

```

The answer matches exact calculation almost perfectly. 

### (v)

@fig-stab show the sampling distribution of the stable population N estimate. 

```{r}
#| fig-cap: "Sampling distribution of stable population N estiamte" 
#| label: fig-stab
N_hat_sampling_dist_plot <- function(df, est_iter){
  
  res <- 
    data.frame(
      i = seq(1,est_iter, 1), 
      N_Stable = rep(NA, est_iter)
    )
  
  for(i in 1:est_iter){
    iter_bh_lm <- lm(I(1/R) ~ I(1/S), data = df[sample(rownames(df), replace = T), ])
    
    res$N_Stable[i] = (1 - coef(iter_bh_lm)[2])/coef(iter_bh_lm)[1]
  }
  
  ggplot(data = res, 
         aes(x = N_Stable)) + 
    
    geom_histogram(bins = 30, color = "black", fill = "grey") + 
    theme_minimal() + 
    ggtitle(paste0("Sampling distribution of stable N \n  based on ", est_iter, " bootstrapp iterations")) + 
    
    xlab("Stable popualtion number estiamte") + 
    ylab("")
}

N_hat_sampling_dist_plot(df = fish, est_iter = 1000)

```

Using this distribution and its quantiles we can obtain a percentile bootstrap confidence intervals. 

The result is given below after the code chunk with a function 

```{r}
#| echo: true
N_hat_percentile_boot <- function(df, est_iter) {

  res <- 
    data.frame(
      i = seq(1,est_iter, 1), 
      N_Stable = rep(NA, est_iter)
    )
  
  for(i in 1:est_iter){
    iter_bh_lm <- lm(I(1/R) ~ I(1/S), data = df[sample(rownames(df), replace = T), ])
    
    res$N_Stable[i] = (1 - coef(iter_bh_lm)[2])/coef(iter_bh_lm)[1]
  }
  
  return(quantile(res$N_Stable, c(0.025, 0.975)))
}

N_hat_percentile_boot(df = fish, est_iter = 1000)

```
