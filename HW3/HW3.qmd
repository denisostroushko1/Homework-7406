---
title: "Homework 3"
author: "Denis Ostroushko"
format: 
  pdf:
    toc: false
    number-sections: false
    colorlinks: true
editor: source
execute: 
  eval: true
  echo: false
  message: false
  warning: false
---

```{R}

#| include: false

library(tidyverse)
library(kableExtra)

```

# Probem 1

Logistic regression model for fitted data is given by: 

$$\Large logit(P(Cancer = Yes)) = -7 + 0.1 * A + 1.2 * S + 0.3 * R + 0.2 * R*S$$

**$YS$ conditional odds ratio equation**

Conditional $YS$ odds ratio is presented when we compare $R=1$ to $R=0$ and let $S$ be a variable in the 
resulting odds ratio. Then, varying levels of smoking will further change odds ratio for $R=1$ vs $R=0$. 

$$\Large OR(R | S = s) = \frac{\frac{P(R = 1 | S = s)}{1 - P(R = 1| S=s)}}{\frac{P(R = 0 | S = s)}{1 - P(R = 0 | S = s)}} = $$

Odds ratio for both numerator and denominator simplify to a single exponential term. We hold A constant while
adjusting for it in our comparison. We let S = s be an arbitrary value of S that takes on value 0 or 1.  

$$\Large \frac{exp(-7 + 0.1 * A + 1.2 * s + 0.3 * 1 + 0.2 *s * 1)}{exp(-7 + 0.1 * A + 1.2 * s + 0.3 * 0 + 0.2 * s*0) } = $$

$$\Large exp(0.3 + 0.2 * s)$$

This odds ratio is the compares the effects of race on the likelihood of having cancer, while adjusting for 
smoking. For black smokers, we have the highest chance of getting cancer, and white non-smokers have the lowest chance of getting cancer. 

More precisely, black non-smokers are $exp(0.3) =$ `r round(exp(0.3), 4)` times more likely to have cancer, 
while black smokers are $exp(0.3 + 0.2) =$ `r round(exp(0.3+0.2),4)` times more likely to have cancer, 
after adjusting for other variables. 

**$YR$ conditional odds ratio equation**

Conditional $YR$ odds ratio is presented when we compare $S=1$ to $S=0$ and let $R$ be a variable in the 
resulting odds ratio. Then, varying levels of smoking will further change odds ratio for $S=1$ vs $S=0$. 

$$\Large OR(S | R = r) = \frac{\frac{P(S = 1 | R = r)}{1 - P(S = 1| R=r)}}{\frac{P(S = 0 | R = r)}{1 - P(S = 0 | R = r)}} = $$

$$\Large = \frac{exp(-7 + 0.1 * A + 1.2 + 0.3*r + 0.2 * 1 * r)}{exp(-7 + 0.1*A + 1.2 * 0 + 0.3*r + 0.2*0*r)}$$

$$\Large exp(1.2 + 0.2 * r) $$

So, smokers are $exp(1.2) =$ `r round(exp(1.2),4)` times more likely to have cancer when compared with non-smokers, after adjusting for other variables. Additionally, black smokers are $exp(1.2 + 0.2) =$ `r round(exp(1.2 + 0.2),4)` times more likely to have cancer, after adjusting for other variables. 

MORE TO FINISH THE PROBLEM 

# Problem 2

```{r}
prob2 <- 
  read.csv("http://jaredhuling.org/data/pubh7406/table_6_3_data.csv")

# summary(factor(prob2$M))

# summary(factor(prob2$E))

# summary(factor(prob2$P))

prob2 <- 
  prob2 %>% 
  
  mutate(
    M_num = ifelse(M == "divorced", 0, 1)
  )

```

Stage 3 model summary table is: 

```{r}

prob2_glm <- glm(M_num ~ E * P + G, data = prob2, family = binomial("logit"))

g_main <- summary(prob2_glm)$coefficients[rownames(summary(prob2_glm)$coefficients) == "GMale"][1]
g_se   <- summary(prob2_glm)$coefficients[rownames(summary(prob2_glm)$coefficients) == "GMale"][2]
g_p_val  <- summary(prob2_glm)$coefficients[rownames(summary(prob2_glm)$coefficients) == "GMale"][4]

inter_main <- summary(prob2_glm)$coefficients[rownames(summary(prob2_glm)$coefficients) == "Eyes:Pyes"][1]
inter_se   <- summary(prob2_glm)$coefficients[rownames(summary(prob2_glm)$coefficients) == "Eyes:Pyes"][2]
inter_p_val  <- summary(prob2_glm)$coefficients[rownames(summary(prob2_glm)$coefficients) == "Eyes:Pyes"][4]

summary(prob2_glm)$coefficients %>% 
  data.frame() %>% 
  kable(booktabs = T)
  
```

change all E, P, G,etc... to their real names for easier reading 

**Effect of G**

interpret effects of `r g_main` and use `r g_se` to get confidence intervals 
if independent then their interaction must be non-significant 

According to comments from TA we also need to state every

test it: 

1. Null hypothesis: $H_0:$ $\hat \beta_{G} = 0$

1.1 Mull hyp in english 

2. Alternative hypothesis: $H_a:$ $\hat \beta_{G} \neq 0$

2.1 Alt hyp in english 

3. Z statistic: $(\frac{\hat \beta - 0}{se(\hat \beta)})$ = `r round((g_main/g_se), 4)`

4. P-value: `r round(g_p_val, 4)`

5. Conclusion: There is enough statistical evidence to conclude that the effect 

**Independence of E and P**

if independent then their interaction must be non-significant 

test it: 

1. Null hypothesis: $H_0:$ $\hat \beta_{E \ and \ P} = 0$

2. Alternative hypothesis: $H_a:$ $\hat \beta_{E \ and \ P} \neq 0$

3. Z statistic: $(\frac{\hat \beta - 0}{se(\hat \beta)})$ = `r round((inter_main/inter_se), 4)`

4. P-value: `r round(inter_p_val, 4)`

5. Conclusion: Effects of $E$ and $P$ are not independent of each, as evidenced by the low p-value and big 
z-statistic. Therefore, we can conclude that effects of variable $E$ have varying effects on the outcome 
$M$, depending on the levels of variable $P$, after adjusting for other variables. 

# Problem 3

```{r}
prob3 <- 
  read.csv("https://jaredhuling.org/data/pubh7406/hiroshima.csv")
```

### (i)


```{R}

prob3$radiation <- factor(prob3$radiation, 
                          levels = unique(prob3$radiation))

prob3_glm <- glm(formula = cbind(leukemia = prob3$leukemia, 
                    other = prob3$other) ~ prob3$radiation , family = binomial(link = "logit"))

summary(prob3_glm)$coefficients %>% 
  data.frame() %>% 
  
  mutate(comps = rownames(summary(prob3_glm)$coefficients), 
         comps = substr(comps, nchar("prob3$") + 1, max(nchar(comps))), 
         
         comps = case_when(
           comps == "cept)" ~ "Intercept", 
           TRUE ~ comps
         )) %>% 
  
  select(comps, everything()) %>% 

  kable(booktabs = T, row.names = FALSE, digits = 3, 
        align = c("l", rep('c', length(length(summary(prob3_glm)$coefficients %>% data.frame())))), 
        col.names = c("Radiation Level", "Estimate", "Std. Error", "Z-value", "P-value"), 
        ) %>% 
   kable_styling(position = "center")

```

INTERPRET ON ODDS SCALE, SO USE EXPOENENTIATED COEFFICIENTS

### (ii)

### (iii)

```{r}

prob3_glm.2 <- glm(formula = cbind(leukemia = prob3$leukemia, 
                    other = prob3$other) ~ prob3$radiation_midpoint , family = binomial(link = "logit"))

summary(prob3_glm.2)$coefficients %>% 
  data.frame() %>% 
  
  mutate(comps = rownames(summary(prob3_glm.2)$coefficients), 
         comps = substr(comps, nchar("prob3$") + 1, max(nchar(comps))), 
         
         comps = case_when(
           comps == "cept)" ~ "Intercept", 
           TRUE ~ comps
         )) %>% 
  
  select(comps, everything()) %>% 

  kable(booktabs = T, row.names = FALSE, digits = 3, 
        align = c("l", rep('c', length(length(summary(prob3_glm.2)$coefficients %>% data.frame())))), 
        col.names = c("Radiation Level", "Estimate", "Std. Error", "Z-value", "P-value"), 
        ) %>% 
   kable_styling(position = "center")

```

COMPARE to model prob3_glm 

Prefer this one: 

becuase claerly lienar trend is better 

Avoid multiple comparisons 
```{R}

viz_data <- 
  data.frame(
    prob = prob3$leukemia / (prob3$leukemia + prob3$other), 
    seqn = seq(from = 1, to = length(unique(prob3$radiation_midpoint)), by = 1), 
    labels = sort(unique(prob3$radiation_midpoint))
  )

viz_data$log_odds <- with(viz_data, log(prob/(1-prob)))

ggplot(data = viz_data, 
       aes(x = seqn, 
           y = log_odds)) + geom_point() + geom_line() + 
  geom_smooth(method = "lm", se = F)

```

**Deviance Comparison**

```{r}

anova(prob3_glm)

anova(prob3_glm.2)
      
anova(prob3_glm, prob3_glm.2, test='LR')

```

**Residual Plots**

```{r}

## somehow our factor model does not produce deviance residuals!! WHYYYY?? 

rbind(
  cbind(
    prob3 %>%
    select(radiation_midpoint, radiation),
    data.frame(Residuals = residuals(prob3_glm, "deviance"), 
               Model = rep("Categorical", nrow(prob3)))
    ),
  cbind(
    prob3 %>%
    select(radiation_midpoint, radiation),
    data.frame(Residuals = residuals(prob3_glm.2, "deviance"), 
               Model = rep("Midpoint", nrow(prob3)))
    )
) %>% 

ggplot(aes(x = radiation_midpoint, 
           y = Residuals, 
           color = Model)) + geom_point(size = 2)


```

### (iv)


```{r recreate pivoted data to a long single obs data set}

# recreate the data as one row per observation from the summary data 

mid_0_leuk_y <- 
  data.frame(
    radiation_midpoint = rep(0, 13), 
    leuk_flag = rep(1, 13)
  )

mid_0_leuk_n <- 
  data.frame(
    radiation_midpoint = rep(0, 378), 
    leuk_flag = rep(0, 378)
  )


mid_5_leuk_y <- 
  data.frame(
    radiation_midpoint = rep(5, 5), 
    leuk_flag = rep(1, 5)
  )

mid_5_leuk_n <- 
  data.frame(
    radiation_midpoint = rep(5, 200), 
    leuk_flag = rep(0, 200)
  )

mid_29_leuk_y <- 
  data.frame(
    radiation_midpoint = rep(29.5, 5), 
    leuk_flag = rep(1, 5)
  )

mid_29_leuk_n <- 
  data.frame(
    radiation_midpoint = rep(29.5, 151), 
    leuk_flag = rep(0, 151)
  )


mid_74_leuk_y <- 
  data.frame(
    radiation_midpoint = rep(74.5, 3), 
    leuk_flag = rep(1, 3)
  )

mid_74_leuk_n <- 
  data.frame(
    radiation_midpoint = rep(74.5, 47), 
    leuk_flag = rep(0, 47)
  )



mid_149_leuk_y <- 
  data.frame(
    radiation_midpoint = rep(149.5, 4), 
    leuk_flag = rep(1, 4)
  )

mid_149_leuk_n <- 
  data.frame(
    radiation_midpoint = rep(149.5, 31), 
    leuk_flag = rep(0, 31)
  )

	
mid_249_leuk_y <- 
  data.frame(
    radiation_midpoint = rep(249.5, 18), 
    leuk_flag = rep(1, 18)
  )

mid_249_leuk_n <- 
  data.frame(
    radiation_midpoint = rep(249.5, 33), 
    leuk_flag = rep(0, 33)
  )


prob3_big_data <- 
  rbind(mid_249_leuk_n,
        mid_249_leuk_y,
        mid_149_leuk_n, 
        mid_149_leuk_y, 
        mid_74_leuk_n, 
        mid_74_leuk_y, 
        mid_29_leuk_n,
        mid_29_leuk_y,
        mid_5_leuk_n,
        mid_5_leuk_y,
        mid_0_leuk_n,
        mid_0_leuk_y
        )

```

```{r fit a log reg model on a single observation data set}

prob3_glm.long_data <- glm(leuk_flag ~ radiation_midpoint, data = prob3_big_data, family = binomial(link = "logit"))
```

```{r helper functions for Odds ratios}
odds <- 
  function(x){ x / (1-x)}
```

Given a logistic regression model with an intercept and one predictor, odds ratio for a one unit change in 
predictor $X$ are given by $\Large e^{\hat \beta_1 * ((x + 1) - x)}= e^{\hat \beta_1}$

Therefore, for two values of $X$ that are more than one unit apart, denoted as $W_1$ and $W_2$ are 
given by $\Large e^{\hat \beta_1 * (W_1 - W_2)}$

Using a full notation and including intercepts we have $\Large e^{\hat \beta_0 - \hat \beta_0 + \hat \beta_1 * (W_1 - W_2)}$

Now we can take a ratio of odds ratios, keeping intercepts in the notation. We denote levels of $X$ from the 
first odds ratio as $W_1$ and $W_2$, and levels of $X$ from the second odds ratio as $Z_1$ and $Z_2$. 
In each case we compare odds for level with subscript 1 to level with subscript 2. 

We then take a ratio of odds ratio for $W's$ to odds ratio for $Z's$. Estimator is given below: 

$$\Large e^{\hat \beta_0 (1 - 1 - 1 + 1) + \hat \beta_1 * (W_1 - W_2 - Z_1 + Z_2)}$$

We can see that algebraic signs follow a patter, and we have one real number as a multiplier for each 
model parameters. Note that a real number for $\hat \beta_0$ is zero, however, it is convenient to 
keep it there as for the derivation of a matrix form calculation. 

Therefore, as per Jared's tip, a ratio of odds ratios is a function of four values of $X$, and can be 
represented as $$\Large e^{\textbf a^T \boldsymbol{\hat \beta}}$$, where $\textbf a^T = [1-1-1+1, W_1 - W_2 - Z_1 + Z_2]$

```{r ratio of odds ratio hand calculation}

## matrix calculation ~~by hand~~ 
#### last line is ratio of ODDS ratios

levels = c(100, 199, 50, 99)

mult = c(1, -1, -1, 1)

betas = coef(prob3_glm.2)

contr_vec = c(sum(mult), sum(mult * levels))
    
## Keep in mind that these are the matrix notation for the power of exp, so these need to be squared

    # t(contr_vec) %*% betas # result of aT * beta hat 
    exp_ratio_odds_ratio <-  exp(t(contr_vec) %*% betas) # final answer
    
    ## so, by Jared's tip, variance is supposed to be calculated using this formula:
    
    var_aT_B <- t(contr_vec) %*% vcov(prob3_glm.2) %*% contr_vec # result of aT * VCOV * aT

## now let's use model predict and construct odds ratio 
#### predict a model using a value of ratioation exposure
#### turn probability into odds, 
#### then gets odds ratio 
#### then get ratio of odds ratios 

p_100 <- predict(prob3_glm.long_data, newdata = data.frame(radiation_midpoint = 100), type = "response")
p_199 <- predict(prob3_glm.long_data, newdata = data.frame(radiation_midpoint = 199), type = "response")

p_50 <- predict(prob3_glm.long_data, newdata = data.frame(radiation_midpoint = 50), type = "response")
p_100 <- predict(prob3_glm.long_data, newdata = data.frame(radiation_midpoint = 100), type = "response")

# ratio of odds ratios 
alternative_ratio_odds_ratios <- 
  (odds(p_100) / odds(p_199))/
  (odds(p_50) / odds(p_100) )

```


So, the ratio of the odds of having leukemia comparing a radiation level of ‘100 to 199‘ and a radiation level of ‘50 to 99‘ is given by
$\Large e^{\hat \beta_0 (1 - 1 - 1 + 1) + \hat \beta_1 * (100 - 199 - 50 + 99)} =$ `r round(exp_ratio_odds_ratio, 6)`

### (v)

In order to obtain a confidence interval for the ratio of odds ratio we can take two approaches: 

1. Calculate confidence interval for $\large \textbf a^T \boldsymbol{\hat \beta}$, and then exponentiate 
  the interval 
  
2. Use the delta method to calculate $\large Var(e^{\textbf a^T \boldsymbol{\hat \beta}})$ and then 
  calculate the 95% confidence interval using a standard error of the odds scale directly. 
  
For my own reference, I will use both methods, and validate that the results indeed match. 

**Log odds confidence interval** 

Using Jared's tip, we can calculate $\large Var(\textbf a^T \boldsymbol{\hat \beta})$ directly by taking 
$\large a^T \textbf V a$ where $V$ is a variance-covariance matrix of the fitted logistic regression model. 
Variance-covariance estimates are given for model estimates including the intercept. 

Using `R` output we estimate $\large Var(\textbf a^T \boldsymbol{\hat \beta}) =$ `r round(var_aT_B, 6)`

Then, the 95% confidence interval on the original scale is $\large \textbf a^T \boldsymbol{\hat \beta} \pm 1.96 * \sqrt{Var(\textbf a^T \boldsymbol{\hat \beta})} =$ `r round(t(contr_vec) %*% betas, 6)` $\pm 1.96 *$ `r round(sqrt(var_aT_B), 6)`

Taking exponential of interval end point given us a confidence interval for the ratio of odds ratios. The 
95% confidence interval is (`r round(exp(t(contr_vec) %*% betas - 1.96 * sqrt(var_aT_B)),6)`, `r round(exp(t(contr_vec) %*% betas + 1.96 * sqrt(var_aT_B)),6)`)


<!-- 
ALERT, ALERT!!! 

For Quarto chunk options to work we need to specify them on the line RIGHT AFTER the ```{R} line 
Otherwise, knitter ignores the option

--> 

```{r  bootstrap simmulation to get variance of odds ratios}
#| eval: false

set.seed(1763712)

simmulate_OR_ci <- 
  function(data, W1, W2, Z1, Z2, iter){
    
    res <- 
      data.frame(iteration = seq(from = 1, to = iter, by = 1), 
                 beta = NA,
                 ratio_odds_ratios = NA)
    
    for(i in 1:iter){
      
      if(i %% 100 == 0){print(paste("on iteration:", i))}
    # resample the data 
      resampeld <- data[sample(rownames(data), size = nrow(data), replace = T),]
      
    # fit the model 
      glm_iter <- glm(leuk_flag ~ radiation_midpoint, data = resampeld, family = binomial("logit"))
      
    # store the estiamte 
      res$beta[i] = coef(glm_iter)[2]
      
      res$ratio_odds_ratios[i] <- 
        (odds(predict(glm_iter, newdata = data.frame(radiation_midpoint = W1), type = "response"))/
          odds(predict(glm_iter, newdata = data.frame(radiation_midpoint = W2), type = "response"))) /
        
        (odds(predict(glm_iter, newdata = data.frame(radiation_midpoint = Z1), type = "response"))/
          odds(predict(glm_iter, newdata = data.frame(radiation_midpoint = Z2), type = "response")))
    }
    
    return(res)
  }

sim_data <- simmulate_OR_ci(data = prob3_big_data, 
                            W1 = 100, 
                            W2 = 199, 
                            Z1 = 50, 
                            Z2 = 100, 
                            iter = 10000)
```



```{r compare results}
#| eval: false

aT_B <- t(contr_vec) %*% betas
var_aT_B <- t(contr_vec) %*% vcov(prob3_glm.2) %*% contr_vec
## calculating variance of exponentiated vector using a delta method
var_exp <- (-50 * exp(-50 * coefficients(prob3_glm.2)[2]))^2 * vcov(prob3_glm.2)[2,2]

# using matrix notation and 
print(paste("Matrix notation estimate:", round(exp(aT_B), 6)))
print(paste("Matrix notation LB exponentiation:", round(exp(aT_B - 1.96 * sqrt(var_aT_B)), 6)))
print(paste("Matrix notation UB exponentiation:", round(exp(aT_B + 1.96 * sqrt(var_aT_B)), 6)))

print("")

print(paste("Matrix notation LB Delta Method:", round(exp(aT_B) - 1.96 * sqrt(var_exp), 6)))
print(paste("Matrix notation UB Delta Method:", round(exp(aT_B) + 1.96 * sqrt(var_exp), 6)))

print("")

print(paste("Bootstrapp Simulation estimate:", round(mean(sim_data$ratio_odds_ratios), 6)))
print(paste("Bootstrapp Simulation LB:", round(quantile(sim_data$ratio_odds_ratios, 0.025), 6)))
print(paste("Bootstrapp Simulation UB:", round(quantile(sim_data$ratio_odds_ratios, 0.975), 6)))

```
