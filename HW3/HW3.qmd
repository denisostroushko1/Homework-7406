---
title: "Homework 3"
author: "Denis Ostroushko"
format: 
  pdf:
    toc: false
    number-sections: false
    colorlinks: true
editor: source
execute: 
  eval: true
  echo: false
  message: false
  warning: false
---

```{R}

#| include: false

library(tidyverse)
library(kableExtra)
library(emmeans)

```

# Probem 1

Logistic regression model for fitted data is given by: 

$$\Large logit(P(Cancer = Yes)) = -7 + 0.1 * A + 1.2 * S + 0.3 * R + 0.2 * R*S$$

**$YS$ conditional odds ratio equation**

Conditional $YS$ odds ratio is presented when we compare $R=1$ to $R=0$ and let $S$ be a variable in the 
resulting odds ratio. Then, varying levels of smoking will further change odds ratio for $R=1$ vs $R=0$. 

$$\Large OR(R | S = s) = \frac{\frac{P(R = 1 | S = s)}{1 - P(R = 1| S=s)}}{\frac{P(R = 0 | S = s)}{1 - P(R = 0 | S = s)}} = $$

Odds ratio for both numerator and denominator simplify to a single exponential term. We hold A constant while
adjusting for it in our comparison. We let S = s be an arbitrary value of S that takes on value 0 or 1.  

$$\Large \frac{exp(-7 + 0.1 * A + 1.2 * s + 0.3 * 1 + 0.2 *s * 1)}{exp(-7 + 0.1 * A + 1.2 * s + 0.3 * 0 + 0.2 * s*0) } = $$

$$\Large exp(0.3 + 0.2 * s)$$

This odds ratio is the compares the effects of race on the likelihood of having cancer, while adjusting for 
smoking. For black smokers, we have the highest chance of getting cancer, and white non-smokers have the lowest chance of getting cancer. 

More precisely, black non-smokers are $exp(0.3) =$ `r round(exp(0.3), 4)` times more likely to have cancer, 
while black smokers are $exp(0.3 + 0.2) =$ `r round(exp(0.3+0.2),4)` times more likely to have cancer, 
after adjusting for other variables. 

**$YR$ conditional odds ratio equation**

Conditional $YR$ odds ratio is presented when we compare $S=1$ to $S=0$ and let $R$ be a variable in the 
resulting odds ratio. Then, varying levels of smoking will further change odds ratio for $S=1$ vs $S=0$. 

$$\Large OR(S | R = r) = \frac{\frac{P(S = 1 | R = r)}{1 - P(S = 1| R=r)}}{\frac{P(S = 0 | R = r)}{1 - P(S = 0 | R = r)}} = $$

$$\Large = \frac{exp(-7 + 0.1 * A + 1.2 + 0.3*r + 0.2 * 1 * r)}{exp(-7 + 0.1*A + 1.2 * 0 + 0.3*r + 0.2*0*r)}$$

$$\Large exp(1.2 + 0.2 * r) $$

So, smokers are $exp(1.2) =$ `r round(exp(1.2),4)` times more likely to have cancer when compared with non-smokers, after adjusting for other variables. Additionally, black smokers are $exp(1.2 + 0.2) =$ `r round(exp(1.2 + 0.2),4)` times more likely to have cancer, after adjusting for other variables. 

MORE TO FINISH THE PROBLEM 

# Problem 2

```{r}
prob2 <- 
  read.csv("http://jaredhuling.org/data/pubh7406/table_6_3_data.csv")

# summary(factor(prob2$M))

# summary(factor(prob2$E))

# summary(factor(prob2$P))

prob2 <- 
  prob2 %>% 
  
  mutate(
    M_num = ifelse(M == "divorced", 0, 1)
  )

```

Stage 3 model summary table given in @tbl-stage

<!-- hallelujah this reference in Quarto PDF files works hell yaaaassss brotheeeeeeer -->

```{r}
#| label: tbl-stage
#| tbl-cap: Summary of the model 

prob2_glm <- glm(M_num ~ E * P + G, data = prob2, family = binomial("logit"))

g_main <- summary(prob2_glm)$coefficients[rownames(summary(prob2_glm)$coefficients) == "GMale"][1]
g_se   <- summary(prob2_glm)$coefficients[rownames(summary(prob2_glm)$coefficients) == "GMale"][2]
g_p_val  <- summary(prob2_glm)$coefficients[rownames(summary(prob2_glm)$coefficients) == "GMale"][4]

inter_main <- summary(prob2_glm)$coefficients[rownames(summary(prob2_glm)$coefficients) == "Eyes:Pyes"][1]
inter_se   <- summary(prob2_glm)$coefficients[rownames(summary(prob2_glm)$coefficients) == "Eyes:Pyes"][2]
inter_p_val  <- summary(prob2_glm)$coefficients[rownames(summary(prob2_glm)$coefficients) == "Eyes:Pyes"][4]

summary(prob2_glm)$coefficients %>% 
  data.frame() %>% 
  kable(booktabs = T) %>% 
  kable_styling(position = "center", latex_options = c("hold_position"))
  
```

change all E, P, G,etc... to their real names for easier reading 

**Effect of G**

interpret effects of `r g_main` and use `r g_se` to get confidence intervals 
if independent then their interaction must be non-significant 

According to comments from TA we also need to state every

test it: 

1. Null hypothesis: $H_0:$ $\hat \beta_{G} = 0$

1.1 Mull hyp in english 

2. Alternative hypothesis: $H_a:$ $\hat \beta_{G} \neq 0$

2.1 Alt hyp in english 

3. Z statistic: $(\frac{\hat \beta - 0}{se(\hat \beta)})$ = `r round((g_main/g_se), 4)`

4. P-value: `r round(g_p_val, 4)`

5. Conclusion: There is enough statistical evidence to conclude that the effect 

**Independence of E and P**

if independent then their interaction must be non-significant 

test it: 

1. Null hypothesis: $H_0:$ $\hat \beta_{E \ and \ P} = 0$

2. Alternative hypothesis: $H_a:$ $\hat \beta_{E \ and \ P} \neq 0$

3. Z statistic: $(\frac{\hat \beta - 0}{se(\hat \beta)})$ = `r round((inter_main/inter_se), 4)`

4. P-value: `r round(inter_p_val, 4)`

5. Conclusion: Effects of $E$ and $P$ are not independent of each, as evidenced by the low p-value and big 
z-statistic. Therefore, we can conclude that effects of variable $E$ have varying effects on the outcome 
$M$, depending on the levels of variable $P$, after adjusting for other variables. 

# Problem 3

```{r}
prob3 <- 
  read.csv("https://jaredhuling.org/data/pubh7406/hiroshima.csv")
```

### (i)

Before fitting the model we can evaluate proportion of leukemia cases in the population for each level of radiation exposure. We can see that the presence of leukemia is similar for the lowest three levels, and 
starts to increase fast the higher exposure levels go. 

```{r}
#| label: leuk-summary
#| tbl-cap: Obseved rates of leukemia by radiation exposure group 

prob3 %>% 
  mutate(
    N = leukemia + other, 
    leukemia_prop = paste0(round(leukemia/(leukemia + other),4) * 100, "%")
  ) %>% 
  
  select(radiation, radiation_midpoint, N, leukemia_prop) %>% 

  kable(booktabs = T, row.names = FALSE, 
      align = 'c', 
      col.names = c("Radiation Level", "Midpoint", "N Observations","Leukemia Proportion"), 
      ) %>% 
 kable_styling(position = "center", latex_options = c("hold_position"))


```

We can fit the model with exposure levels as categorical predictor. The model will provide comparisons of 
each level to the baseline level, which had no exposure. We can then use pairwise comparisons to evalaute a 
large family of comparisons. 

```{R}
#| label: prob3-m1
#| tbl-cap: Model estimates with radiation level as a categorical predictor

prob3$radiation <- factor(prob3$radiation, 
                          levels = unique(prob3$radiation))

prob3_glm <- glm(formula = cbind(leukemia = prob3$leukemia, 
                    other = prob3$other) ~ prob3$radiation , family = binomial(link = "logit"))

summary(prob3_glm)$coefficients %>% 
  data.frame() %>% 
  
  mutate(comps = rownames(summary(prob3_glm)$coefficients), 
         comps = substr(comps, nchar("prob3$") + 1, max(nchar(comps))), 
         
         comps = case_when(
           comps == "cept)" ~ "Intercept", 
           TRUE ~ comps
         ), 
         
         odds_ratio = exp(Estimate)) %>% 
  
  select(comps, Estimate, odds_ratio, everything()) %>% 

  kable(booktabs = T, row.names = FALSE, digits = 3, 
        align = c("l", rep('c', length(length(summary(prob3_glm)$coefficients %>% data.frame())))), 
        col.names = c("Radiation Level", "Estimate", "Odds Ratio",  "Std. Error", "Z-value", "P-value"), 
        ) %>% 
   kable_styling(position = "center", latex_options = c("hold_position"))

comp_1 <-  round(exp((summary(prob3_glm)$coefficients %>% data.frame() %>% select(Estimate) )[5, ]), 4)
comp_1_ci <- paste0("(", paste(round(exp(confint(prob3_glm)[5,]), 4), collapse = ", "), ")")

comp_2 <-  round(exp((summary(prob3_glm)$coefficients %>% data.frame() %>% select(Estimate) )[6, ]), 4)
comp_2_ci <- paste0("(", paste(round(exp(confint(prob3_glm)[6,]), 4), collapse = ", "), ")")
  
```

* Looking at a small set of comparisons of each level of exposure to the referense level, people who 
got 100-200 and 200+ units of exposure are at a much higher chance of developing leukemia, compared with those 
who had no exposure to radiation. 

* We are not able to conclude that the log odds of the event are different from the baseline for other levels of 
exposure. 

* The odds of having leukemia for those who got between 100 and 199 units of radiation when compared with those 
who got no exposure are `r comp_1` times higher, bounded by the `r comp_1_ci` 95% confidence interval. 

  Confidence interval does not include 1, having all values consistently above 1, therefore we have enough 
  statistical evidence to conclude that those who got between 100 and 199 units of exposure are at a much higher 
  risk of developing leukemia. 

* Similarly, the odds of developing leukemia for those who got 200+ units of exposure compared to baseline 
level are `r comp_2` times higher, bounded by the `r comp_2_ci` 95% confidence interval. 

In the next section we evaluate pairwise comparisons of levels of exposure. 

### (ii)

```{r}

em <- emmeans(prob3_glm, "radiation")
contrast(em, "pairwise") %>% data.frame() -> comparisons

```

We can perform `r nrow(comparisons)` pairwise comparisons for our model. 

Family Wise Error Rate is given as $P(At \ least \ one \ false \ positive) = 1 - P(no \ false \ rejections) = 1 - 0.95^{15} =$ `r round(1 - 0.95^15, 4)`.

All possible comparisons are given below, the table is ordered from the lowest to the highest p-value: 

```{r}
#| label: pw-unadjusted
#| tbl-cap: Pairwise unadjusted comparisons of odds ratios between radiation exposure levels

comparisons %>% 
  arrange(p.value) %>% 
  select(-df, -z.ratio, -SE)  %>% 
  mutate(odds_ratio = exp(estimate)) %>% 
  select(contrast, estimate, odds_ratio, p.value) -> comparisons_f 

comparisons_f %>% 
  kable(booktabs = T, row.names = FALSE, digits = 3, 
        align = c("l", rep('c', length(comparisons_f) - 1)), 
        col.names = c("Contrast", "Estimate", "Odds Ratio", "P-value")
        ) %>% 
   kable_styling(position = "center", latex_options = c("hold_position"))
  

```

Without controling for the multiple comparison, or multiple testing, error, we have `r length(which(comparisons$p.value < 0.05))` statistically significant comparisons at the 0.05 significance level.  
**Holm-Bonferroni Adjustment** 

In order to control FWER for `r nrow(comparisons)` comparisons we need to use the Holm - Bonferroni Stepdown Procedure. We present a table with contrasts one more time, including Holm adjusted p-values this time. 

```{R}
#| label: pw-holm 
#| tbl-cap: Pairwise comparions of odds ratios adjusted using Holm procedure

comparisons_f$holm_adj <- p.adjust(comparisons_f$p.value, method = "holm")

comparisons_f %>% 
  kable(booktabs = T, row.names = FALSE, digits = 3, 
        align = c("l", rep('c', length(comparisons_f) - 1)), 
        col.names = c("Contrast", "Estimate", "Odds Ratio", "P-value", "Holm Adjusted P-value")
        ) %>% 
   kable_styling(position = "center", latex_options = c("hold_position"))

```

Comments: 

* How FWER adjustments prevents us from concluding one of the differeces is there 

moving on

**Benjamini-Hochberg Adjustment** 

In order to control FDR for `r nrow(comparisons)` comparisons we need to use the Benjamini-Hochberg Adjustment Procedure. We present a table with contrasts one more time, including Holm adjusted p-values this time. 

```{R}
#| label: pw-bh
#| tbl-cap: Pairwise comparions of odds ratios adjusted using BH procedure

comparisons_f$hoch_adj <- p.adjust(comparisons_f$p.value, method = "BH")

comparisons_f <- comparisons_f %>% select(-holm_adj)

comparisons_f %>% 
  kable(booktabs = T, row.names = FALSE, digits = 3, 
        align = c("l", rep('c', length(comparisons_f) - 1)), 
        col.names = c("Contrast", "Estimate", "Odds Ratio", "P-value", "BH Adjusted P-value")
        ) %>% 
   kable_styling(position = "center", latex_options = c("hold_position"))

```

Comments: 

* Confirm that obserbed differences withiut adjustments are kind of true differences. 

move on 

### (iii)

First, let's fit the model that uses a midpoint radiation exposure as a continuous predictor of the log-odds 
of developing leukemia. Being able to estimate and extrapolate chances of developing leukemia given some 
other levels of radiation exposure is a big advantage over a model with categorical predictors.

Model is given 

```{r}
#| label: mid-point-model
#| tbl-cap: Model estimates with radiation midpoint level as a continuous  predictor

prob3_glm.2 <- glm(formula = cbind(leukemia = prob3$leukemia, 
                    other = prob3$other) ~ prob3$radiation_midpoint , family = binomial(link = "logit"))

summary(prob3_glm.2)$coefficients %>% 
  data.frame() %>% 
  
  mutate(comps = rownames(summary(prob3_glm.2)$coefficients), 
         comps = substr(comps, nchar("prob3$") + 1, max(nchar(comps))), 
         
         comps = case_when(
           comps == "cept)" ~ "Intercept", 
           TRUE ~ comps
         )) %>% 
  
  select(comps, everything()) %>% 

  kable(booktabs = T, row.names = FALSE, digits = 3, 
        align = c("l", rep('c', length(length(summary(prob3_glm.2)$coefficients %>% data.frame())))), 
        col.names = c("Radiation Level", "Estimate", "Std. Error", "Z-value", "P-value"), 
        ) %>% 
   kable_styling(position = "center", latex_options = c("hold_position"))

```

* We can see that the p-value is low and Z-statistic value is large, so we have enough statistical evidence 
that radiation exposure is linearly related to the chance of developing leukemia. As radiation exposure levels
increase, the chance gets higher. 

We should use a linear model over a categorical model for a number of reasons. First, as stated above, is the 
ability to extrapolate the odds of developing leukemia at levels other than those presented in the data, 
but without going too far outside of the model scope. For example, we should not use this model to 
estimate the proportion of leukemia among other levels at 500 units of radiation exposure. 

Another reason is the relationship between midpoints and log odds of developing leukemia. 
I took observed proportions of leukemia from the data and transformed them to the log-odds scale. 

```{R}
#| label: raw_odds_plot
#| fig-cap: Observed log-odds ratios at midpoint radiation exposure 

viz_data <- 
  data.frame(
    prob = prob3$leukemia / (prob3$leukemia + prob3$other), 
    seqn = seq(from = 1, to = length(unique(prob3$radiation_midpoint)), by = 1), 
    labels = sort(unique(prob3$radiation_midpoint))
  )

viz_data$log_odds <- with(viz_data, log(prob/(1-prob)))

ggplot(data = viz_data, 
       aes(x = seqn, 
           y = log_odds)) + geom_point() + geom_line() + 
  geom_smooth(method = "lm", se = F) + 
  theme_minimal() + 
  
  ylab("Log-Odds of developing leukemia") + 
  xlab("Radiation exposure midpoint") + 
  
  ggtitle("Relationship between radiation exposure and \n the chance of developing leukemia") + 
  scale_x_continuous(breaks = seq(from = 1, to = max(viz_data$seqn), by = 1),  
                                  labels = as.factor(viz_data$labels))

```

* As we can see, there a clear linear trend that can be modeled using a regression approach with a continuous 
predictor. 

**Deviance Comparison**

We can also compare the two models using a likelihood ratio test. 

```{r}
#| label: deviance 
#| tbl-cap: Comparison of Deviance statistics for the two candidate models 

anova(prob3_glm, prob3_glm.2, test='LR') %>% 
  data.frame() %>% 
  mutate(model = c("Categorical", "Midpoint")) %>% 
  select(model, everything()) %>% 
  kable(booktabs = T, 
        align = c("l", rep('c', length(anova(prob3_glm, prob3_glm.2, test='LR') %>% data.frame()))), 
        digits = 2
          ) %>% 
  kable_styling(position = 'center', latex_options = c('hold_position'))

```

* Evidently, a model with a continuous predictor has lower deviance, however, the drop in deviance is 
not statistically significant, so we can't conclude that using a continuous predictor lower deviance drastically.

**Residual Plots**

We can finally compare residuals for the two models. I use `R` to calculate and store pearson residuals for 
two models, and display residual for each model at each respective level. Note that I used radiation
midpoint on the x-axis. However, there is a one-to-one relationship between bucketed levels of ration 
exposure and a midpoint for the bucket. Therefore, it is appropriate to use midpoint as a label. 

```{r}
#| label: residuals 
#| fig-cap:  Pearson residuals for the two candidate models 

## somehow our factor model does not produce deviance residuals!! WHYYYY?? 

rbind(
  cbind(
    prob3 %>%
    select(radiation_midpoint, radiation),
    data.frame(Residuals = residuals(prob3_glm, "deviance"), 
               Model = rep("Categorical", nrow(prob3)), 
               seqn = seq(from = 1, to = nrow(prob3), by = 1))
    ),
  cbind(
    prob3 %>%
    select(radiation_midpoint, radiation),
    data.frame(Residuals = residuals(prob3_glm.2, "deviance"), 
               Model = rep("Midpoint", nrow(prob3)), 
               seqn = seq(from = 1, to = nrow(prob3), by = 1))
    )
) %>% 

ggplot(aes(x = seqn, 
           y = Residuals, 
           color = Model)) + geom_point(size = 2) +
  scale_x_continuous(breaks = seq(from = 1, to = nrow(prob3), by = 1), 
                     labels = unique(prob3$radiation_midpoint)) + 
  theme_minimal() + 
  ylab("Pearson Residuals") + 
  xlab("Radiation Midpoint/Corresponding group of radiation exposure level") + 
  ggtitle("Comparison of residuals for two models")


```
* As we can see, a model with categorical predictors essentially does not produce any residuals. 
This is reasonable because we fit a model using 6 observations, and fit one proportion per observation. 

* However, a model with a midpoint continuous predictor shows behavior that is more expected of a regression model. We can see how our fit tends to underestimate log-odds of getting leukemia. 

* Indeed, we saw of the figure above that, perhaps, a fit with the second order exponential may be more 
appropriate. 

### (iv)

```{r recreate pivoted data to a long single obs data set}

# recreate the data as one row per observation from the summary data 

mid_0_leuk_y <- 
  data.frame(
    radiation_midpoint = rep(0, 13), 
    leuk_flag = rep(1, 13)
  )

mid_0_leuk_n <- 
  data.frame(
    radiation_midpoint = rep(0, 378), 
    leuk_flag = rep(0, 378)
  )


mid_5_leuk_y <- 
  data.frame(
    radiation_midpoint = rep(5, 5), 
    leuk_flag = rep(1, 5)
  )

mid_5_leuk_n <- 
  data.frame(
    radiation_midpoint = rep(5, 200), 
    leuk_flag = rep(0, 200)
  )

mid_29_leuk_y <- 
  data.frame(
    radiation_midpoint = rep(29.5, 5), 
    leuk_flag = rep(1, 5)
  )

mid_29_leuk_n <- 
  data.frame(
    radiation_midpoint = rep(29.5, 151), 
    leuk_flag = rep(0, 151)
  )


mid_74_leuk_y <- 
  data.frame(
    radiation_midpoint = rep(74.5, 3), 
    leuk_flag = rep(1, 3)
  )

mid_74_leuk_n <- 
  data.frame(
    radiation_midpoint = rep(74.5, 47), 
    leuk_flag = rep(0, 47)
  )



mid_149_leuk_y <- 
  data.frame(
    radiation_midpoint = rep(149.5, 4), 
    leuk_flag = rep(1, 4)
  )

mid_149_leuk_n <- 
  data.frame(
    radiation_midpoint = rep(149.5, 31), 
    leuk_flag = rep(0, 31)
  )

	
mid_249_leuk_y <- 
  data.frame(
    radiation_midpoint = rep(249.5, 18), 
    leuk_flag = rep(1, 18)
  )

mid_249_leuk_n <- 
  data.frame(
    radiation_midpoint = rep(249.5, 33), 
    leuk_flag = rep(0, 33)
  )


prob3_big_data <- 
  rbind(mid_249_leuk_n,
        mid_249_leuk_y,
        mid_149_leuk_n, 
        mid_149_leuk_y, 
        mid_74_leuk_n, 
        mid_74_leuk_y, 
        mid_29_leuk_n,
        mid_29_leuk_y,
        mid_5_leuk_n,
        mid_5_leuk_y,
        mid_0_leuk_n,
        mid_0_leuk_y
        )

```

```{r fit a log reg model on a single observation data set}

prob3_glm.long_data <- glm(leuk_flag ~ radiation_midpoint, data = prob3_big_data, family = binomial(link = "logit"))
```

```{r helper functions for Odds ratios}
odds <- 
  function(x){ x / (1-x)}
```

Given a logistic regression model with an intercept and one predictor, odds ratio for a one unit change in 
predictor $X$ are given by $\Large e^{\hat \beta_1 * ((x + 1) - x)}= e^{\hat \beta_1}$

Therefore, for two values of $X$ that are more than one unit apart, denoted as $W_1$ and $W_2$ are 
given by $\Large e^{\hat \beta_1 * (W_1 - W_2)}$

Using a full notation and including intercepts we have $\Large e^{\hat \beta_0 - \hat \beta_0 + \hat \beta_1 * (W_1 - W_2)}$

Now we can take a ratio of odds ratios, keeping intercepts in the notation. We denote levels of $X$ from the 
first odds ratio as $W_1$ and $W_2$, and levels of $X$ from the second odds ratio as $Z_1$ and $Z_2$. 
In each case we compare odds for level with subscript 1 to level with subscript 2. 

We then take a ratio of odds ratio for $W's$ to odds ratio for $Z's$. Estimator is given below: 

$$\Large e^{\hat \beta_0 (1 - 1 - 1 + 1) + \hat \beta_1 * (W_1 - W_2 - Z_1 + Z_2)}$$

We can see that algebraic signs follow a patter, and we have one real number as a multiplier for each 
model parameters. Note that a real number for $\hat \beta_0$ is zero, however, it is convenient to 
keep it there as for the derivation of a matrix form calculation. 

Therefore, as per Jared's tip, a ratio of odds ratios is a function of four values of $X$, and can be 
represented as $$\Large e^{\textbf a^T \boldsymbol{\hat \beta}}$$, where $\textbf a^T = [1-1-1+1, W_1 - W_2 - Z_1 + Z_2]$

```{r ratio of odds ratio hand calculation}

## matrix calculation ~~by hand~~ 
#### last line is ratio of ODDS ratios

levels = c(100, 199, 50, 99)

mult = c(1, -1, -1, 1)

betas = coef(prob3_glm.2)

contr_vec = c(sum(mult), sum(mult * levels))
    
## Keep in mind that these are the matrix notation for the power of exp, so these need to be squared

    # t(contr_vec) %*% betas # result of aT * beta hat 
    exp_ratio_odds_ratio <-  exp(t(contr_vec) %*% betas) # final answer
    
    ## so, by Jared's tip, variance is supposed to be calculated using this formula:
    
    var_aT_B <- t(contr_vec) %*% vcov(prob3_glm.2) %*% contr_vec # result of aT * VCOV * aT

## now let's use model predict and construct odds ratio 
#### predict a model using a value of ratioation exposure
#### turn probability into odds, 
#### then gets odds ratio 
#### then get ratio of odds ratios 

p_100 <- predict(prob3_glm.long_data, newdata = data.frame(radiation_midpoint = 100), type = "response")
p_199 <- predict(prob3_glm.long_data, newdata = data.frame(radiation_midpoint = 199), type = "response")

p_50 <- predict(prob3_glm.long_data, newdata = data.frame(radiation_midpoint = 50), type = "response")
p_100 <- predict(prob3_glm.long_data, newdata = data.frame(radiation_midpoint = 100), type = "response")

# ratio of odds ratios 
alternative_ratio_odds_ratios <- 
  (odds(p_100) / odds(p_199))/
  (odds(p_50) / odds(p_100) )

delta_method_var <- (-50 * exp(-50 * betas[2]))^2 * vcov(prob3_glm.2)[2,2]

```


So, the ratio of the odds of having leukemia comparing a radiation level of ‘100 to 199‘ and a radiation level of ‘50 to 99‘ is given by
$\Large e^{\hat \beta_0 (1 - 1 - 1 + 1) + \hat \beta_1 * (100 - 199 - 50 + 99)} =$ `r round(exp_ratio_odds_ratio, 6)`

### (v)

In order to obtain a confidence interval for the ratio of odds ratio we can take two approaches: 

1. Calculate confidence interval for $\large \textbf a^T \boldsymbol{\hat \beta}$, and then exponentiate 
  the interval 
  
2. Use the delta method to calculate $\large Var(e^{\textbf a^T \boldsymbol{\hat \beta}})$ and then 
  calculate the 95% confidence interval using a standard error of the odds scale directly. 
  
  Logistic regression model estimates are asymptotically normally distributed as a result of MLE estimation,
  so we can directly apply the delta method, and obtain another normally distributed random variable.
  
  
  
For my own reference, I will use both methods, and validate that the results indeed match. 

**Transformation of confidence interval bounds** 

Using Jared's tip, we can calculate $\large Var(\textbf a^T \boldsymbol{\hat \beta})$ directly by taking 
$\large a^T \textbf V a$ where $V$ is a variance-covariance matrix of the fitted logistic regression model. 
Variance-covariance estimates are given for model estimates including the intercept. 

Using `R` output we estimate $\large Var(\textbf a^T \boldsymbol{\hat \beta}) =$ `r round(var_aT_B, 6)`

Then, the 95% confidence interval on the original scale is $\large \textbf a^T \boldsymbol{\hat \beta} \pm 1.96 * \sqrt{Var(\textbf a^T \boldsymbol{\hat \beta})} =$ `r round(t(contr_vec) %*% betas, 6)` $\pm 1.96 *$ `r round(sqrt(var_aT_B), 6)`

Taking exponential of interval end point given us a confidence interval for the ratio of odds ratios. The 
95% confidence interval is (`r round(exp(t(contr_vec) %*% betas - 1.96 * sqrt(var_aT_B)),6)`, `r round(exp(t(contr_vec) %*% betas + 1.96 * sqrt(var_aT_B)),6)`)

**Delta method**

Since $\large \boldsymbol{a^T}$ are constants, and $\large \boldsymbol{\hat \beta}$ is a random variable that 
contain a sampling distribution and variance, we define $\large g(\boldsymbol{\hat \beta})= e^{\boldsymbol{a^T * \hat \beta}}$ in order to find $\large Var(e^{\boldsymbol{a^T * \hat \beta}})$

First step is to take a derivative with respect to $\large \boldsymbol{\hat \beta}$. I will use notation 
that is specific to our problem rather than a general form. 

$\Large \frac{d}{d \hat \beta} g({\hat \beta}) = -50 * e ^{-50 * \hat \beta_1}$, therefore 

$\Large Var(e^{\boldsymbol{a^T * \hat \beta}}) = Var(e^{-50 * \hat \beta}) = (-50 * e ^{-50 * \hat \beta_1})^2 * Var(\hat \beta)$ = `r round(delta_method_var , 6)`

So, we can use a ratio of odds ratio of on the odds scale and calculate confidence interval for this 
estimate using a square root of variance we just obtained. 

So, the 95% confidence interval is given by: `r round(exp(t(contr_vec) %*% betas), 6)` $\pm 1.96 *$ `r round(sqrt(delta_method_var) , 6)`, giving us (`r round(exp(t(contr_vec) %*% betas)- 1.96 * sqrt(delta_method_var),6)`, `r round(exp(t(contr_vec) %*% betas) + 1.96 * sqrt(delta_method_var) ,6)`) 

**Conclusion** 

Both methods produce *almost* identical confidence intervals. An advantage of the delta method is knowing 
variance of the ratios of odds ratios, which gives us a standard error. Sometimes, it is easier to interpret 
and report this quantity to non-statisticians and other professionals. 

<!-- 
ALERT, ALERT!!! 

For Quarto chunk options to work we need to specify them on the line RIGHT AFTER the ```{R} line 
Otherwise, knitter ignores the option

--> 
```{r  bootstrap simmulation to get variance of odds ratios}
#| eval: false

set.seed(1763712)

simmulate_OR_ci <- 
  function(data, W1, W2, Z1, Z2, iter){
    
    res <- 
      data.frame(iteration = seq(from = 1, to = iter, by = 1), 
                 beta = NA,
                 ratio_odds_ratios = NA)
    
    for(i in 1:iter){
      
      if(i %% 100 == 0){print(paste("on iteration:", i))}
    # resample the data 
      resampeld <- data[sample(rownames(data), size = nrow(data), replace = T),]
      
    # fit the model 
      glm_iter <- glm(leuk_flag ~ radiation_midpoint, data = resampeld, family = binomial("logit"))
      
    # store the estiamte 
      res$beta[i] = coef(glm_iter)[2]
      
      res$ratio_odds_ratios[i] <- 
        (odds(predict(glm_iter, newdata = data.frame(radiation_midpoint = W1), type = "response"))/
          odds(predict(glm_iter, newdata = data.frame(radiation_midpoint = W2), type = "response"))) /
        
        (odds(predict(glm_iter, newdata = data.frame(radiation_midpoint = Z1), type = "response"))/
          odds(predict(glm_iter, newdata = data.frame(radiation_midpoint = Z2), type = "response")))
    }
    
    return(res)
  }

sim_data <- simmulate_OR_ci(data = prob3_big_data, 
                            W1 = 100, 
                            W2 = 199, 
                            Z1 = 50, 
                            Z2 = 100, 
                            iter = 10000)
```



```{r compare results}
#| eval: false

aT_B <- t(contr_vec) %*% betas
var_aT_B <- t(contr_vec) %*% vcov(prob3_glm.2) %*% contr_vec
## calculating variance of exponentiated vector using a delta method
var_exp <- (-50 * exp(-50 * coefficients(prob3_glm.2)[2]))^2 * vcov(prob3_glm.2)[2,2]

# using matrix notation and 
print(paste("Matrix notation estimate:", round(exp(aT_B), 6)))
print(paste("Matrix notation LB exponentiation:", round(exp(aT_B - 1.96 * sqrt(var_aT_B)), 6)))
print(paste("Matrix notation UB exponentiation:", round(exp(aT_B + 1.96 * sqrt(var_aT_B)), 6)))

print("")

print(paste("Matrix notation LB Delta Method:", round(exp(aT_B) - 1.96 * sqrt(var_exp), 6)))
print(paste("Matrix notation UB Delta Method:", round(exp(aT_B) + 1.96 * sqrt(var_exp), 6)))

print("")

print(paste("Bootstrapp Simulation estimate:", round(mean(sim_data$ratio_odds_ratios), 6)))
print(paste("Bootstrapp Simulation LB:", round(quantile(sim_data$ratio_odds_ratios, 0.025), 6)))
print(paste("Bootstrapp Simulation UB:", round(quantile(sim_data$ratio_odds_ratios, 0.975), 6)))

```
