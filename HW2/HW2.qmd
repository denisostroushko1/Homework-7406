---
title: "Homework 2"
author: "Denis Ostroushko"
format:
  pdf:
    toc: false
    number-sections: false
    colorlinks: true
editor: source
execute:
  warning: false
  message: false
  echo: false
  
---

```{r set up chunk, echo = F, include = F}

alpha_level = 0.33

source("/Users/denisostroushko/Desktop/UofM MS/MS 2023 - 1 Spring/PUBH 7406/Homework-7406/Package master list .R")

```


# Problem 1

### A

**Interpretation**

The linear probability model with one predictor is given by a general equation $\hat \pi(x) = \hat \beta_0 + \hat \beta_1 * x$. Due to its simple form, the interpretation is straightforward. When x = 0, i.e. a person 
consumes no alcohol, estimated probability of malformation is the intercept, which is 0.0025. 

A coefficient for alcohol consumption is 0.001087, meaning that each additional alcholic drink per day 
increases the prbability of malformation by 0.001087. There are no other predictors in the model, so 
we do not have anything to hold constant in this interpretation.

**Relative Risk**

We can use software output and obtain a fitted model: $\large P(Y = 1|x) = \hat \pi(x) = 0.0025 + 0.001087 *x$

Now we can use this model to estimate the probability of having an event when x = 0 and x = 7. 

$\hat \pi(0) =$ 0.0025

$\hat \pi(7) =$ 0.0025 + 0.001087 * 7 = `r round(0.0025 + 0.001087 * 7, 4)`

Therefore, the relative risk is given by a ratio of fitted probabilities, which is 
$\large \frac{\hat \pi(7)}{\hat \pi(0)} =$ `r round((0.0025 + 0.001087 * 7)/(0.0025),4)` 

### B 

# Problem 2

We define a probability of a even happening for each observation $i$ to be a random quantity $\pi_i = P(Y = 1)$.

A GLM with a log link means that we model the natural parameter $\eta_i = log(\pi_i)$ in terms of a linear 
combination of predictors. 

Therefore, a GLM equation is given as 

$$\Large log(\pi_i) = \hat \beta_0 + \hat \beta_1 * x_1 + \dots + \hat \beta_p * x_p$$
Consider the case of varying just one variable $x_1$ by 1 unit, which can either represent the case of switching 
from one categorical level to the next, or increasing a continuous predictor by 1 unit. 

Changing $x_1$ will change the probability from $\pi_1$ to $\pi_2$, and the difference of two probabilities on the 
logarithmic scale is given by 

$$\Large log(\pi_2) - log(\pi_1) = \hat \beta_0 + \hat \beta_1 * (x_1 + 1) + \dots + \hat \beta_p * x_p - \hat \beta_0 - \hat \beta_1 * x_1 - \dots - \hat \beta_p * x_p => $$

$$\Large \hat \beta_1 = log(\frac{\pi_2}{\pi_1}) $$

Therefore, $\Large \frac{\pi_2}{\pi_1} = e^{\hat\beta_1}$. Taking the ratio instead of a difference of probabilities results in the relative comparison, therefore we evaluate relative risk here. 

We do not use this link function often because of the form that $\hat\pi(x)$ takes on. 
$\hat\pi(x) = e^{\hat \beta_0 + \hat \beta_1 * (x_1 + 1) + \dots + \hat \beta_p * x_p}$ is a function that will 
always be greater than 0 because of the properties of exponential function, but it is not limited by 1 on the 
upped end. So, given the data, we can have a scenario where fitted probabilities are greater than 1, which 
violates axioms of probability. 


# Problem 3

### A

We estimate a general linear logistic regression model using a logit link function. So, taking estimates from the 
table, we know that the software fitted a model that takes this form: 

$$\Large log(\frac{\pi(x)}{1 - \pi(x)}) = -3.7771 + 0.1449 * x$$
Using logit function, we can calculate the probability of remission when LI = 8: 

$$\Large \pi(LI = 8) = \frac{e^{-3.7771 + 0.1449 * 8}}{1 + e^{-3.7771 + 0.1449 * 8}} =>$$ 

$\Large \hat \pi =$ `r round(exp(-3.7771 + 0.1449 * 8)/(1 + exp(-3.7771 + 0.1449 * 8)), 3)`

### B 

In this problem we will fix $\hat \pi$ at 0.5 and solve for LI. 

$$\Large log(\frac{0.5}{1-0.5)}) = -3.7771 + 0.1449 * x =>  \frac{log(\frac{0.5}{(1-0.5)}) + 3.7771}{0.1449} = x =>$$  

$\Large x =$ `r round((log(0.5/0.5) + 3.7771)/0.1449,4)` $\approx$ `r round((log(0.5/0.5) + 3.7771)/0.1449)`

### C

The rate of change in $\pi$ in the case with one predictor is approximated by $\hat \beta * \hat \pi(x) * (1 - \hat \pi(x))$. 

We take $\hat \beta$ = 0.1449,while $\hat \pi(LI = 8) =$ 0.068,  from part (a). So, the rate of change is 0.1449 * 0.068 * (`r round(1-0.068,3)`) = `r round(0.1449 * 0.068 * (1-0.068),3)`


Similarly, the rate of change at LI = 26 is 0.1449 * 0.5 * 0.5 = `r round(0.1449 * 0.5 * 0.5 ,3)`

### D

```{r}

return_prob_from_logit <- 
  function(x){
    
    return(exp(-3.7771 + 0.1449 * x)/(1 + exp(-3.7771 + 0.1449 * x)))
    
  }
```

Using methods from parts (a), (b), (c) we estimate the probability of remission at LI = 14 = 
$\hat \pi(14) = P(Y = 1|LI = 14) =$ `r round(return_prob_from_logit(14), 2)`. 

Probability of remission at LI = 28 is $\hat \pi(28)$ = `r round(return_prob_from_logit(28),2)`. 

Thus, probability increases by `r round(return_prob_from_logit(28) - return_prob_from_logit(14),2)` when 
LI increases from 14 to 28. 

### E

Odds ratio for a logistic regression model is given by $e^{\hat \beta_1}$ for a predictor $x_1$. 
This is the multiplicative change in odds ratio. 

In our problem, $\hat \beta_1$ = 0.1449, and so the odds ratio is $e^{0.1449}$ = `r round(exp(0.1449),2)`

### F

Odds ratio is a function of the model parameter $\hat \beta_1$. This parameter is an MLE estimate, so by the 
invariance property, odds ratio is also an MLE. We know that MLE's are asymptotically normally distributed. 

Therefore, we need to do the following steps to a confidence interval for odds ratio. 

1. Get a 95% confidence interval for $\hat \beta_1$ using 1.96 - 97.5th quantile of the the standard normal distribution and a standard error, which we take from the model output. This is a Wald confidence interval. 

2. we exponentiate the lower limit of a 95% confidence interval, an odds ratio, and an upper limit. 

```{r}
l <- 0.1449 - 1.96 * 0.0593
u <- 0.1449 + 1.96 * 0.0593

or_l <- exp(l)
or <- exp(0.1449)
or_u <- exp(u)
```

Recall that  $\hat \beta_1$ = 0.1449, and the standard error is 0.0593. Therefore, the 95% confidence interval is 
(`r round(l,3)`, `r round(u,3)`). 

Taking an exponential of all three quantities gives us quantities that we are looking for. Odds ratio is 
`r round(or,2)` with a (`r round(or_l,2)`, `r round(or_u,2)`) 95% confidence interval. 

Note that the odds ratio of 1 implies no effect of a predictor on the estimated relapse probability. Obtained 
confidence interval does not contain a 1, all values are above 1, therefore we can conclude that increase in 
LI levels is strongly associated with the chance of relapse. One unit increase in LI multiplies the odds of relapse
by `r round(or,2)`. 

Given a different set of observations, fitting model with the same predictor will produce a different $\hat \beta_1$. We hope that the true value of $\beta_1$ is captured by this confidence interval 95% of the time. 

### G 

In the logistic regression framework, Wald test tells us if the estimate is statistically different from 0

1. Null hypothesis: $H_0: \hat \beta = 0$

2. Alternative hypothesis: $H_a: \hat \beta \neq 0$

3. Test statistic: $\large \frac{\hat \beta - 0}{se(\hat \beta)}$ = $\frac{0.1449}{0.0593}$ = `r round(0.1449/0.0593,3)`

4. Cutoff value is the 97.5th quantile of standard normal distribution = $Z^*$ = 1.96

5. $P(Z^* > Z) =$ `r pnorm(0.1449/0.0593, lower.tail = F)` 

6. P value is small and the test statistic is greater than the cutoff value for significance at the 95% confidence level. Therefore, we have enough evidence to reject the null hypothesis and conclude that the effect of LI level
is not zero. Higher LI levels are positively associated with the chance of relapse. 

### H 

We can conduct a likelihood ratio test for the effect when we compare a model with 1 additional parameter against a model with just the intercept.

1. Null hypothesis: $H_0: \hat \beta = 0$

2. Alternative hypothesis: $H_a: \hat \beta \neq 0$

3. Null deviance: 34.372, Residual deviance: 26.073, Test statistic is $X^2 =$ 34.372 - 26.073 = 8.299

4. Degree of freedom = 1 due to one parameter subject to test 

5. Cutoff for significance is the 95th percentile of a chi-square distribution with 1 degree of freedom: 
`r round(qchisq(p = 0.95, df = 1), 4)`

6. $P(\chi^2_1 > X^2)$ = `r round(pchisq(8.299, df = 1, lower.tail = F), 5)`. 

7. We have enough statistical evidence to reject the null hypothesis and conclude that the estimate is 
different from zero. The drop in deviance is large enough to conclude that the addition of LR levels as a 
predictor is necessary to improve model fit. 

### I 

I decided to adopt an approach from page 30 of lecture notes for part one. I will first find a 
standard error and a confidence interval for the linear predictor, 
$\eta =  \vec x^T \hat \beta$. 
Then we will use a logit function to map linear predictor and a 95% confidence interval to the 
probability space. 

**Variance of a linear predictor**

Software output shows that the intercept and LI parameter are correlated, therefore, variance 
of $\hat \beta_0 + \hat \beta_{LI} * LI$ is given by 

$$\Large Var(\hat \beta_0 + \hat \beta_{LI} * LI) = Var(\hat \beta_0) + LI^2 * Var(\hat \beta_{LI}) + 2 * LI * Cov(\hat \beta_0, \hat \beta_{LI})$$

A variance-covariance matrix shows that $Var(\hat \beta_{LI})$ = 0.003521, $Var(\hat \beta_{0})$
= 1.900616, and $Cov(\hat \beta_0, \hat \beta_{LI})$ = -0.07653. 

I will use functions in R to assist with calculations. Function below produces a linear 
predictor given a set of model parameters: 

```{r}
#| echo: true

# logit function for mapping to probability space 
logit <- 
  function(x){
    exp(x)/(1+exp(x))
  }

# estimate linear combination of predictors 
lin_predictor <- 
  function(beta_0, beta_1, x){
    beta_0 + beta_1 * x
  }

```

We can confirm that when LI = 8, linear predictor $\hat \eta=$ 
`r round(lin_predictor(beta_0 = -3.7771, beta_1 = .1449, x = 8), 4)`, and fitted probability is 
$logit(\hat \eta)$ = `r round(logit(lin_predictor(beta_0 = -3.7771, beta_1 = .1449, x = 8)), 4)`,
which matches our previous results.

Now we can produce variance and standard error for the linear combination of predictors 

```{r}
#| echo: true

# compute variance 
var_lin_predictor <- 
  function(beta_0, beta_0_var, 
           beta_1, beta_1_var, 
           cov, 
           x){
    return( 
      beta_0_var + x^2 * beta_1_var + 2 * x * cov
    )
  }

```

```{r}
eta = (lin_predictor(beta_0 = -3.7771, beta_1 = .1449, x = 8))

var = var_lin_predictor(beta_0 = -3.7771, beta_0_var = 1.900616,beta_1 = 0.1449, beta_1_var = 0.003521, cov = -.07653, x= 8)
```

We estimate that variance of $\hat \eta$ =  is `r var_lin_predictor(beta_0 = -3.7771, beta_0_var = 1.900616,beta_1 = 0.1449, beta_1_var = 0.003521, cov = -.07653, x= 8)`

So, when LI = 8, $\hat \eta$ = `r round(lin_predictor(beta_0 = -3.7771, beta_1 = .1449, x = 8), 4)`  with a (`r round(eta - 1.96*sqrt(var), 4)`, `r round(eta + 1.96 * sqrt(var), 4)`)
95% confidence interval. 

**Confidence interval of a fitted probability**

Now, using a logit function, we can map these three estimates to the probability space. 

Estiamted probability of relapse at LI = 8 is `r round(logit(eta), 4)`, bounded by the 
(
`r round(logit(eta - 1.96*sqrt(var)), 4)`,
`r round(logit(eta + 1.96*sqrt(var)), 4)`
) 95% confidence interval. 

# Problem 4

# Problem 5

### A 

We know that odds ratio = $e^{\hat \beta}$, therefore we need an inverse of this function 
to get $\hat \beta$, i.e. $\hat \beta = log(Odds \ Ratio)$. 


I will use code below to assit me in calculations: 

```{r}
#| echo: true

# some differnece 
get_std_err_from_ub <- 
  function(or, or_ub){
    
    return(
      (log(or_ub) - log(or))/1.96
    )
    
  }

get_std_err_from_lb <- 
  function(or, or_lb){
    
    return(
      (log(or) - log(or_lb))/1.96
    )
    
  }
```

**Some Education vs No Education Predictor**

1. Odds Ratio is 4.04, so $\hat \beta_{Education} = log(4.04)$ = `r round(log(4.04),4)`

2. To get a standard error, we will take the difference between log(OR) and the lower/upper 
bound of C.I., and divide that difference by 1.96. We do that for both lower and upper bounds 
to make sure that we get the same value from each calculation, which will be a standard error 
for the model estimate

 * Thus, scaled difference between log(OR) and log(Lower Bound of C.I) is 
$\frac{log(4.04) - log(1.17)}{1.96} =$ `r round(get_std_err_from_lb(or = 4.04, or_lb = 1.17),4)`
 
 * Scaled difference between log(Upper Bound of C.I) and log(OR) is 
$\frac{log(13.9) - log(4.04)}{1.96} =$ `r round(get_std_err_from_ub(or = 4.04, or_ub = 13.9),4)`

 * We have a small rounding error, but the two calculations agree, therefore 
 $\hat \beta_{Education}=$ `r round(log(4.04),4)` with $se(\beta_{Education})$ = `r round(get_std_err_from_ub(or = 4.04, or_ub = 13.9),4)`

**Gender** 

Standard error from upper bound: `r get_std_err_from_ub(or = 1.38, or_ub = 12.88)`

Standard error from lower bound: `r get_std_err_from_lb(or = 1.38, or_lb = 1.23)`

Not equal, address in (b)

**SES High vs Low**

**Number of Partners**

### B 

take 1.38 to be a log-odds ratio, so we need to exponentiate and then take the log again to get the right 
estiamte

So, calculating from the upper bound, we have `r get_std_err_from_ub(or = exp(1.38), or_ub = 12.88)`

So, calculating from the lower bound, we have `r get_std_err_from_lb(or = exp(1.38), or_lb = 1.23)`

# Problem 6

